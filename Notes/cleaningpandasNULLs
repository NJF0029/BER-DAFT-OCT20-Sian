1. Revisit wrangling task list and discuss nulls 
a) filter the data / select the subset of rows or columns you need 
c) amend or set data types
d) ensure data in those columns is formatted consistently with the data type 
e) deal with duplicate rows 
f) deal with null values / null handling 

What do we know about NULLs ? 
Anyone know who invented the null ? Give you 1 minute to find out 
What about someone with the surname NULL 

What about NULL long and lat - what can go wrong 
I found this example in a book called humble pi by matt Parker


https://www.codeproject.com/Articles/787668/Why-We-Should-Love-null

Why do we need to be careful with managing nulls? 

Anyone offer a good example of when nulls should possibly be kept. 

Suppose you wanted to find an apartment for rent in Berlin. How many apartments are listed at EUR0? Does this mean the apartment is free ? Or that someone replaced the null (unknown) value with a 0 ? Ok we know the apartment isn’t free. But when you look at average apartment prices in an area - is the 0 included ? The truth is you shouldn’t ignore or modify a null without thinking through the consequences 

What about if you log into your bank via your mobile app and see the balance 0 instead of a null pointer 

Q What’s a null device ? 

2. Set up for activity 1

Now that we have thought about our NULLs lets get back to getting rid of them 

Keeping in mind that the time spent wrangling a column should be justified by the value obtained from the data… I have two questions about our nulls
*  is a column worth keeping there are loads of NULLs ? What’s a sensible threshold ?
* Can we logically replace the nulls with something else? 
A lot of businesses adopt a 70/30 rule of thumb.  
Maybe need to do some data profiling then…lets find out how many nulls we have - by column. Lets have a look at the data - what would you estimate the % nulls per column ? 
... Would be great if theres an easy way to compute this 

round(data.isna().sum()/len(data),4)*100 

Next steps for creating a table with the profile results

nulls_df = pd.DataFrame(round(data.isna().sum()/len(data),4)*100)
nulls_df = nulls_df.reset_index()
nulls_df.columns = ['header_name', 'percent_nulls']
nulls_df

how to drop columns based on % nulls as needed (dummy example with 3%), filter data frame based on nulls in a column, or filter null values from certain columns to clean the data 

columns_drop = nulls_df[nulls_df['percent_nulls']>3]['header_name’]  
# dummy case with 3 
print(columns_drop.values)

 Nulls in numerical columns: Some ways to approach the problem

* Ignore these observations
* Replace with general average
* Replace with similar type of averages
* Build model to predict missing values

data[data['gender'].isna()==True] # checking rows that are null based on a specific column 

mean_median_home_value = np.mean(data['median_home_val'])
data['median_home_val'] = data['median_home_val'].fillna(mean_median_home_value)

Replacing null values for categorical variables -General Approaches:
    
* Ignore observation
* Replace by most frequent value
* Replace using an algorithm like KNN using the neighbours.
* Predict the observation using a multiclass predictor.
* Treat missing data as just another category
data['gender'].value_counts()

Number of missing values 
len(data[data['gender'].isna()==True]) 

Fill with the most populous value - data['gender'] = data['gender'].fillna('F’) 
BUT should we do this? 

How to spit the data we have cleaned out to a file for future use :
data.to_csv('merged_clean_ver1.csv')

Activity 1 - students work with data frame and manage nulls out of data where they find any 
